{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cosima10/master-thesis/blob/main/keyword_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DP2WSlNRmqw"
      },
      "source": [
        "---\n",
        "\n",
        "## Preparation\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4VBFjyLEq4"
      },
      "source": [
        "#### <span style=\"color:pink\"> USE THIS MODULE IF YOU ARE RUNNING IT IN GOOGLE COLAB </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LatzIlSM-6vg",
        "outputId": "870fc5c9-b80b-48c4-c254-ef6b11f61073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmjYjDb1_yAr",
        "outputId": "a74c82ae-1e3a-49ca-93a9-848f3f0171d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.2.0-py3-none-any.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.6 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 40.7 MB/s \n",
            "\u001b[?25hCollecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.2.0)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.2.0 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "\u001b[31mERROR: Invalid requirement: 'selenium,'\u001b[0m\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,992 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,512 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,286 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,799 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,021 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,231 kB]\n",
            "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 13.2 MB in 3s (3,867 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 89.8 MB of archives.\n",
            "After this operation, 302 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 101.0.4951.64-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 101.0.4951.64-0ubuntu0.18.04.1 [78.5 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 101.0.4951.64-0ubuntu0.18.04.1 [4,980 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 101.0.4951.64-0ubuntu0.18.04.1 [5,153 kB]\n",
            "Fetched 89.8 MB in 4s (23.7 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_101.0.4951.64-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_101.0.4951.64-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (101.0.4951.64-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: target 'usr/bin' is not a directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: use options instead of chrome_options\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "!pip install selenium\n",
        "from selenium import webdriver\n",
        "import sys\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "! pip install selenium, webdriver_manager\n",
        "! apt-get update\n",
        "! apt install chromium-chromedriver\n",
        "! cp / usr/lib/chromium-browser/chromedriver / usr/bin\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "driver = wd\n",
        "import os\n",
        "\n",
        "path_source = \"/content/drive/MyDrive/scraper/df link.xlsx\"\n",
        "path_output = \"/content/drive/MyDrive/scraper/result/\"\n",
        "if not os.path.exists(path_output):\n",
        "    os.makedirs(path_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jmM5cwSLEq6"
      },
      "source": [
        "#### <span style=\"color:pink\"> USE THIS MODULE IF YOU ARE RUNNING IT ON YOUR LOCAL MACHINE </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v2hf6RoiLEq7"
      },
      "outputs": [],
      "source": [
        "# !pip install selenium, webdriver_manager, numpy, pandas\n",
        "# import time\n",
        "# import numpy as np\n",
        "# from bs4 import BeautifulSoup\n",
        "# import pandas as pd\n",
        "# import sys\n",
        "# from selenium import webdriver\n",
        "# from webdriver_manager.chrome import ChromeDriverManager\n",
        "# import os\n",
        "# chrome_options = webdriver.ChromeOptions()\n",
        "# chrome_options.add_argument('--headless')\n",
        "# chrome_options.add_argument('--no-sandbox')\n",
        "# chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "# wd = webdriver.Chrome(ChromeDriverManager().install(), options=chrome_options)\n",
        "# driver = wd\n",
        "# cwd = os.getcwd()\n",
        "# cwd\n",
        "\n",
        "# path_source = cwd+\"/df link.xlsx\"\n",
        "# path_output = cwd+\"/result/\"\n",
        "# if not os.path.exists(path_output):\n",
        "#     os.makedirs(path_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# SCRAPER\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QPCXqcQUKYWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3hs8sLae-ASO"
      },
      "outputs": [],
      "source": [
        "# initialising string\n",
        "\n",
        "def hapusspecialcharacter(ini_string):\n",
        "    getVals = list([val for val in ini_string\n",
        "                    if val.isalpha() or val.isnumeric() or val == \" \"])\n",
        "    result = \"\".join(getVals)\n",
        "    return result\n",
        "\n",
        "\n",
        "def unique(list1):\n",
        "    unique_list = []\n",
        "    for x in list1:\n",
        "        if x not in unique_list:\n",
        "            unique_list.append(x)\n",
        "    return(unique_list)\n",
        "\n",
        "\n",
        "def years_check(year, df):\n",
        "    number_of_news_in_year = len(df[df['date'].str.contains(str(year))])\n",
        "    return number_of_news_in_year\n",
        "\n",
        "df_link = pd.read_excel(path_source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mbhXFCMoDfhl"
      },
      "outputs": [],
      "source": [
        "# create output folders and files \n",
        "\n",
        "def createExcelFiles(iprocess, kamus, keyword):\n",
        "    if not os.path.exists(f\"{path_output}/{df_link['country'][iprocess]}\"):\n",
        "        os.makedirs(f\"{path_output}/{df_link['country'][iprocess]}\")\n",
        "\n",
        "    results_path = f\"{path_output}/{df_link['country'][iprocess]}/\"\n",
        "    \n",
        "    df = pd.DataFrame(kamus).astype(str)\n",
        "    file_name = f\"{results_path}Links {df_link['country'][iprocess]} {df_link['number'][iprocess]} {df_link['link'][iprocess].split('/')[2].replace('.','_')}.xlsx\"\n",
        "    # if file exist\n",
        "    if os.path.exists(file_name):   mode = 'a'\n",
        "    else: mode = 'w'\n",
        "    with pd.ExcelWriter(file_name, mode=mode, engine=\"openpyxl\") as writer:\n",
        "        df.to_excel(writer, sheet_name=keyword, index=False)\n",
        "\n",
        "    yearList = []\n",
        "    counterList = []\n",
        "    for i in range(2023)[-6:]:\n",
        "        yearList.append(i)\n",
        "        counterList.append(years_check(i, df))\n",
        "\n",
        "    file_nameCek = f\"{results_path}Stats {df_link['country'][iprocess]} {df_link['number'][iprocess]} {df_link['link'][iprocess].split('/')[2].replace('.','_')}.xlsx\"\n",
        "    kamusCek = {\"year\": yearList, \"Number of Related News\": counterList}\n",
        "    dfCek = pd.DataFrame(kamusCek).astype(str)\n",
        "    if os.path.exists(file_nameCek):   mode = 'a'\n",
        "    else: mode = 'w'\n",
        "    with pd.ExcelWriter(file_nameCek, mode=mode, engine=\"openpyxl\") as writer:\n",
        "        dfCek.to_excel(writer, sheet_name=keyword, index=False)\n",
        "    return dfCek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6J3d8G4LEq9"
      },
      "source": [
        "---\n",
        "\n",
        "## DECORATOR\n",
        "replacing the common part of each scraper and therewith reducing redundancy\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FvjbSEHdLEq9"
      },
      "outputs": [],
      "source": [
        "def scrap_data(params):\n",
        "    def scrap_decorator(fn):\n",
        "        tick = time.time()\n",
        "        iprocess = params.get('iprocess')\n",
        "        url = params.get('url')\n",
        "        pagination = params.get('pagination')\n",
        "\n",
        "        def wrapper(*args, **kwargs):\n",
        "            keyword = args[0] if args else kwargs.get('keyword', None)\n",
        "            news_links = []\n",
        "            dates = []\n",
        "            for ipage in pagination:\n",
        "                wd.get(url.format(keyword=keyword, ipage=ipage))  # link with search query\n",
        "                content = driver.page_source.encode('utf-8').strip()\n",
        "                soup = BeautifulSoup(content)\n",
        "                news_links, dates, found = fn(soup, ipage, news_links, dates)\n",
        "                if not found:\n",
        "                    print(f\"[PAGE NO : {ipage}] [NO MORE ARTICLES FOUND]\")\n",
        "                    break\n",
        "                \n",
        "            kamus = {\"news link\": news_links, \"date\": dates}\n",
        "            print(f'[Website: {url.split(\"/\")[2]}] [Keyword: {keyword}] [Time Taken: {round(time.time() - tick, 2)} sec]')\n",
        "            return createExcelFiles(iprocess, kamus, keyword)\n",
        "\n",
        "        return wrapper\n",
        "    return scrap_decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVujCXi2LEq-"
      },
      "source": [
        "---\n",
        "\n",
        "# GERMANY\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJewdJNwLEq-"
      },
      "source": [
        "#### 1) NTV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uAiITkCSLEq-"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 7, 'url': 'https://www.ntv.de/suche/?q={keyword}&at=all&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def germany1ntv(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find('div', class_='search__results').find_all(\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", \"teaser__date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xRUJiyuLEq-",
        "outputId": "cc9e0751-2a6c-449b-fd83-b1927063e588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAGE NO : 1] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 2] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 3] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 4] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 5] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 6] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 7] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 8] [ARTICLES FOUND: 20]\n",
            "[PAGE NO : 9] [ARTICLES FOUND: 14]\n",
            "[PAGE NO : 10] [NO MORE ARTICLES FOUND]\n",
            "   year Number of Related News\n",
            "0  2017                      3\n",
            "1  2018                     91\n",
            "2  2019                     34\n",
            "3  2020                     15\n",
            "4  2021                     23\n",
            "5  2022                      7\n"
          ]
        }
      ],
      "source": [
        "#TEST \n",
        "#print(germany1ntv(\"dsgvo\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY-U2g2ULEq_"
      },
      "source": [
        "#### 2) Deutsche Welle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3ythOlN_LEq_"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 9, 'url': 'https://www.dw.com/search/?languageCode=en&item={keyword}&searchNavigationId=9097&sort=DATE&resultsCounter=10000', 'pagination': list(range(1,21))})\n",
        "def germany2dw(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", \"searchResult\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", \"date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVOALH_SLEq_"
      },
      "source": [
        "#### 3) Frankfurter Allgemeine Zeitung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gsYf6VugLEq_"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 10, 'url': 'https://www.faz.net/suche/s{ipage}.html?ct=article&ct=audio&ct=blog&ct=gallery&ct=infografik&ct=storytelling&ct=video&&query={keyword}#listPagination', 'pagination': list(range(1,21))})\n",
        "def germany3faz(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"li\", \"lst-Teaser_Item\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1qy8np-LErA"
      },
      "source": [
        "---\n",
        "\n",
        "# DENMARK\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vv9LTQQLErA"
      },
      "source": [
        "#### 1) INFORMATION.DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UH67LvcoLErA"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 0, 'url': 'https://www.information.dk/search/site/{keyword}?page={ipage}', 'pagination': list(range(1,21))})\n",
        "def denmark1information(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", {\"typeof\": \"sioc:Item foaf:Document\"})\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.information.dk\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"li\", \"date first\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USDXvOpZLErA"
      },
      "source": [
        "#### 2) POLITIKEN.DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bgwTGazHLErA"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 1, 'url': 'https://politiken.dk/search/?q={keyword}&target=pol&sort=pd&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def denmark2politiken(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", \"search-result__article u-padding--vertical-normal\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://politiken.dk/\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfelc1fiLErA"
      },
      "source": [
        "#### 3) BERLINGSKE.DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5GmQh2qLErB"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 4, 'url': 'https://www.berlingske.dk/search?query={keyword}&limit=10&offset={ipage}0', 'pagination': list(range(1,21))})\n",
        "def denmark3berlingske(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", \"teaser__description align-self-top w-100\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.berlingske.dk/\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"div\",\"teaser__date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yet7Xff2LErB"
      },
      "source": [
        "#### 4) JV.DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKCOWlsCLErB"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 2, 'url': 'https://jv.dk/soeg?keywords={keyword}&limit=10&offset={ipage}0', 'pagination': list(range(1,21))})\n",
        "def denmark4jv(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find('div', class_='page-block').find_all(\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://jv.dk/\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\", \"search-result__date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Ry3m7YLErB"
      },
      "source": [
        "---\n",
        "\n",
        "# AUSTRIA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pybEF5oLErB"
      },
      "source": [
        "#### 1) WIENER ZEITUNG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MnZN0ZGTLErB"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 23, 'url': 'https://www.wienerzeitung.at/suche/?such={keyword}&date_min=20220112&date_max=20220412&em_page={ipage}', 'pagination': list(range(1,21))})\n",
        "def austria1wienerzeitung(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", \"col-12 col-md-4 em_mobile_list_layout\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "411mzmuGLErC"
      },
      "source": [
        "#### 2) KLEINE ZEITUNG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1tj7B9csLErC"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 24, 'url': 'https://www.kleinezeitung.at/user/search.do?action=1&resultsPage={ipage}&resetForm=0&type=4196%2C4193%2C4206&searchText={keyword}', 'pagination': list(range(1,21))})\n",
        "def austria2kleinezeitung(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find('section', class_='article-card--search').find_all(\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "        try:\n",
        "            dates.append(\" \".join(i.find(\"small\", \"article-card__date\").text.split()))\n",
        "        except: dates.append(\" \".join(\"\"))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k45FCf0gLErC"
      },
      "source": [
        "#### 3) DIE PRESSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N-PFQG9ALErC"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 26, 'url': 'https://www.diepresse.com/suche?s={keyword}&p={ipage}', 'pagination': list(range(1,21))})\n",
        "def austria3diepresse(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", \"card__content\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            try:    dates.append(\" \".join(i.find(\"div\", \"card__header\").get_text().split()))\n",
        "            except: dates.append(\"\")\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeLvKYb_LErC"
      },
      "source": [
        "---\n",
        "\n",
        "# PORTUGAL\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBHBy46vLErC"
      },
      "source": [
        "#### 1) OBSERVADOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2emZA-NoLErC"
      },
      "outputs": [],
      "source": [
        "def portugal1observador(keyword):\n",
        "    news_links = []\n",
        "    dates = []\n",
        "    iprocess = 31\n",
        "    print(\"[WEBSITE: OBSERVADOR]\")\n",
        "    wd.get(f\"https://observador.pt/pesquisa/?q={keyword}\")    \n",
        "    for ipage in list(range(21))[1:]:\n",
        "        content = driver.page_source.encode('utf-8').strip()\n",
        "        soup = BeautifulSoup(content)\n",
        "        b = soup.find_all(\"div\", \"gsc-webResult gsc-result\")\n",
        "        print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "        for i in b:\n",
        "            link = i.find(\"a\")\n",
        "            if link:\n",
        "                news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "                try:\n",
        "                    dates.append(\" \".join(i.find(\"div\", \"gs-visibleUrl-breadcrumb\").find_all('span')[1].text.split()))\n",
        "                except:\n",
        "                    dates.append(\"\")\n",
        "                try:\n",
        "                    wd.find_element_by_xpath('//*[@id=\"___gcse_0\"]/div/div/div/div[5]/div[2]/div/div/div[2]/div/div[ipage]'.replace(\"ipage\", str(ipage))).click()\n",
        "                    time.sleep(2)\n",
        "                except:\n",
        "                    y = 0\n",
        "    kamus = {\"news link\": news_links, \"date\": dates}\n",
        "    return createExcelFiles(iprocess, kamus, keyword)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_dDLY5BLErC"
      },
      "source": [
        "#### 2) DN.PT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yd7wb-N_LErD"
      },
      "outputs": [],
      "source": [
        "def portugal2dn(keyword):\n",
        "    news_links = []\n",
        "    dates = []\n",
        "    iprocess = 30\n",
        "    print(\"[WEBSITE: DN]\")\n",
        "    wd.get(f\"https://www.dn.pt/pesquisa.html?q={keyword}\")    \n",
        "    for ipage in list(range(21))[1:]:\n",
        "        content = driver.page_source.encode('utf-8').strip()\n",
        "        soup = BeautifulSoup(content)\n",
        "        b = soup.find_all(\"div\", \"gsc-webResult gsc-result\")\n",
        "        print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "        for i in b:\n",
        "            link = i.find(\"a\")\n",
        "            if link:\n",
        "                news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "                try:\n",
        "                    dates.append(\" \".join(i.find(\"div\", \"gs-bidi-start-align gs-snippet\").text.split('...')[0].split()))\n",
        "                except:\n",
        "                    dates.append(\"\")\n",
        "                try:\n",
        "                    time.sleep(1)\n",
        "                    wd.find_element_by_xpath('//*[@id=\"___gcse_0\"]/div/div/div/div[5]/div[2]/div/div/div[2]/div/div[ipage]'.replace(\"ipage\", str(ipage))).click()\n",
        "                    time.sleep(2)\n",
        "                except:\n",
        "                    y = 0\n",
        "    kamus = {\"news link\": news_links, \"date\": dates}\n",
        "    return createExcelFiles(iprocess, kamus, keyword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Cw-id3LErD"
      },
      "source": [
        "#### 3) DNOTICIAS.PT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FIxCNyYiLErD"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 32, 'url': 'https://pesquisa.dnoticias.pt/elastic/results/?search={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def portugal3dnoticias(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find('ul', class_='search-list-results').find_all('li')\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", \"search-result-date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2MQkzb4LErD"
      },
      "source": [
        "---\n",
        "\n",
        "# ITALY\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zch0E-c5LErD"
      },
      "source": [
        "#### 1) REPUBBLICA.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0PwSpcV-LErD"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 33, 'url': 'https://ricerca.repubblica.it/ricerca/repubblica?query={keyword}&view=repubblica&ref=HRHS&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def italy1repubblica(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS3BOE0QLErD"
      },
      "source": [
        "#### 2) TODAY.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cWPmccCcLErD"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 40, 'url': 'https://www.today.it/search/query/{keyword}/pag/{ipage}', 'pagination': list(range(1,21))})\n",
        "def italy2today(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"span\", class_='c-story__byline').find('a').get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_='c-story__byline').text.split(',')[-1].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpadBZq8LErE"
      },
      "source": [
        "#### 3) ILPOST.IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VIxNc3vfLErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 39, 'url': 'https://www.ilpost.it/search_gcse/?q={keyword}#gsc.tab=0&gsc.q={keyword}&gsc.page={ipage}', 'pagination': list(range(1,21))})\n",
        "def italy3ilpost(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", class_='gsc-webResult gsc-result')\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find('a').get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"div\", class_='gs-visibleUrl-breadcrumb').find_all('span')[1].text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKTJPISNLErE"
      },
      "source": [
        "---\n",
        "\n",
        "# BULGARIA\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) VESTI.BG"
      ],
      "metadata": {
        "id": "SZ-llpy6Lkxf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HqUHsslmLErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 65, 'url': 'https://www.vesti.bg/tarsene?q={keyword}', 'pagination': list(range(1,2))})\n",
        "def bulgaria1vesiti(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", class_=\"list-item list-item-category\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find('a').get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_=\"label-time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) 24CHASA.BG"
      ],
      "metadata": {
        "id": "0a2Ear6mLouO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ihkXxz7WLErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 66, 'url': 'https://www.24chasa.bg/Search?what={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def bulgaria2chasa(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", class_=\"entry-short\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find('a').get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_=\"article-date\").text.split('|')[-1].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) BLITZ.BG"
      ],
      "metadata": {
        "id": "vBuS3HqVLrq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0pMYeTpHLErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 67, 'url': 'https://blitz.bg/search?q={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def bulgaria3blitz(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all(\"div\", class_=\"tech-news-content mt-0\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find('a').get(\"href\"))\n",
        "            dates.append(\" \".join(i.find_all(\"p\")[-1].find(\"span\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) FAKTI.BG"
      ],
      "metadata": {
        "id": "o6RGxWPrLwgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-svftlhzLErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 68, 'url': 'https://fakti.bg/search?q={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def bulgaria4fakti(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find('section', id='main').find_all(\"li\")[:21]\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://fakti.bg\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"div\", class_=\"ndtvc\").find(\"div\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZbnUfguLErE"
      },
      "source": [
        "---\n",
        "# CROATIA\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) INDEX.HR"
      ],
      "metadata": {
        "id": "KlD_xzxNL0aX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "QAraJyH0LErE"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 58, 'url': 'https://www.index.hr/trazi.aspx?upit={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def croatia1index(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"grid-items-list\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.index.hr\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"div\", class_=\"publish-date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) TPORTAL.HR"
      ],
      "metadata": {
        "id": "3l_Kaa2xL3t0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Cis_3dazLErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 62, 'url': 'https://www.tportal.hr/pretrazivanje?query={keyword}&page={ipage}', 'pagination': list(range(1,21))})\n",
        "def croatia2tportal(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"articlePreviewListType10\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.tportal.hr\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"h2\", class_=\"title publishedSort\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) DNEVNO.HR"
      ],
      "metadata": {
        "id": "dT-_RcMpL7Uq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "azYw5K0ELErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 64, 'url': 'https://www.dnevno.hr/?s={keyword}', 'pagination': list(range(1,2))})\n",
        "def croatia3dnevno(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('article', class_=\"post\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\", class_=\"date\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QKL0WLlLErF"
      },
      "source": [
        "---\n",
        "# Greece\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) PROTOTHEMA.GR"
      ],
      "metadata": {
        "id": "c1lBErdjL-WA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "brEQBYOhLErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 49, 'url': 'https://www.protothema.gr/ajax/Atcom.Sites.ProtoThema.Components.Search.Page/?pg={ipage}&q={keyword}', 'pagination': list(range(1,21))})\n",
        "def greece1protothema(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_=\"update_well\").text.split(',')[0].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) IN.GR"
      ],
      "metadata": {
        "id": "8_hmi7YvMDpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NWN0QLICLErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 51, 'url': 'https://www.in.gr/search-results/3/?q={keyword}#gsc.tab=0&gsc.q={keyword}&gsc.page={ipage}', 'pagination': list(range(1,21))})\n",
        "def greece2in(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"gsc-webResult gsc-result\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"div\", class_=\"gs-bidi-start-align gs-snippet\").text.split('...')[0].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) NEWSIT.GR"
      ],
      "metadata": {
        "id": "pspMJLlbMHuz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "rIN2_p44LErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 53, 'url': 'https://www.newsit.gr/page/{ipage}/?s={keyword}', 'pagination': list(range(1,21))})\n",
        "def greece3newsit(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('article', class_=\"blog-article\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\", class_=\"entry-date published\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) DIKAILOGIKA.GR"
      ],
      "metadata": {
        "id": "5GNvG9fWMLFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0bthQ3CLErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 52, 'url': 'https://www.dikaiologitika.gr/site/search/{keyword}?searchword={keyword}&start={ipage}', 'pagination': list(map(lambda x: x*20,range(21)))})\n",
        "def greece4dikaiologitika(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"itemContainer\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.dikaiologitika.gr\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_=\"genericItemDateCreated\").text.split('-')[0].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtYpyzRtLErF"
      },
      "source": [
        "---\n",
        "# SLOVAKIA\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1) TOPKY.SK"
      ],
      "metadata": {
        "id": "XeaNStWkMPkc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2UB9qBZuLErF"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 44, 'url': 'https://www.topky.sk/search/{keyword}/{ipage}/', 'pagination': list(range(1,21))})\n",
        "def slovakia1topky(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"entry\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.topky.sk\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"p\", class_=\"time\").text.split(' ')[0].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) TERAZ.SK"
      ],
      "metadata": {
        "id": "8UwnC_Q6MTon"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ixuDfmubLErG"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 47, 'url': 'https://www.teraz.sk/search?q={keyword}&p={ipage}', 'pagination': list(range(1,21))})\n",
        "def slovakia2teraz(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('div', class_=\"mediaListing-item\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"https://www.teraz.sk\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"time\").text.split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) AKTUALITY.SK"
      ],
      "metadata": {
        "id": "5wNtpSbKMWq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "HTwBuFeHLErG"
      },
      "outputs": [],
      "source": [
        "@scrap_data(params={'iprocess': 42, 'url': 'https://www.aktuality.sk/vyhladavanie/{ipage}/?search%5Btext%5D={keyword}&search%5Bzdroj%5D=spravy', 'pagination': list(range(1,21))})\n",
        "def slovakia3aktuality(soup, ipage, news_links, dates):\n",
        "    found = True\n",
        "    b = soup.find_all('li', class_=\"article-item list-item\")\n",
        "    if len(b) == 0: found = False\n",
        "    else: print(f\"[PAGE NO : {ipage}] [ARTICLES FOUND: {len(b)}]\")\n",
        "    for i in b:\n",
        "        link = i.find(\"a\")\n",
        "        if link:\n",
        "            news_links.append(\"\"+i.find(\"a\").get(\"href\"))\n",
        "            dates.append(\" \".join(i.find(\"span\", class_=\"article-time\").text.split(' ')[0].split()))\n",
        "    return news_links, dates, found"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# OUTPUT \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ct29-JMnMalz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2DTNXNFAKJ4"
      },
      "source": [
        "Each time, one will run the function for a certain news site of a country, it will output two excel files with the following data:\n",
        "\n",
        "- Excel file 1: number of news articles found for each keyword per year (2017-2022), each keyword is in a different sheet within the same excel file,\n",
        "- Excel file 2: links of news articles per date (for cross-checks) separated for each keyword in a different sheet.\n",
        "\n",
        "Naming conventions for the files that you will find in the folder 'results' > country_name:\n",
        "\n",
        "- Result + country_name + news_site_name: statistics \n",
        "\n",
        "- country_name + news_site_name: links for cross-checking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# RUN THE CODE\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "C9tH88MWwgts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DK_keyword_list = [\"gdpr\", \"privatliv\", \"Databeskyttelsesloven\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in DK_keyword_list:\n",
        "    print(denmark1information(keyword))\n",
        "    print(denmark2politiken(keyword))\n",
        "    print(denmark3berlingske(keyword))\n",
        "    #print(denmark4jv(keyword))"
      ],
      "metadata": {
        "id": "bMmjLMnidnPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GER_keyword_list = [\"gdpr\", \"dsgvo\", \"datenschutz\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in GER_keyword_list:\n",
        "    print(germany1ntv(keyword))\n",
        "    print(germany2dw(keyword))\n",
        "    print(germany3faz(keyword))"
      ],
      "metadata": {
        "id": "Y0kuPDJnwe3Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AT_keyword_list = [\"gdpr\", \"dsgvo\", \"datenschutz\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in AT_keyword_list:\n",
        "    print(austria1wienerzeitung(keyword))\n",
        "    print(austria2kleinezeitung(keyword))\n",
        "    print(austria3diepresse(keyword))"
      ],
      "metadata": {
        "id": "IVQ74Q9Xdc9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PT_keyword_list = [\"gdpr\", \"Lei n.º 58/2019\", \"privacidade\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in PT_keyword_list:\n",
        "    print(portugal1observador(keyword))\n",
        "    print(portugal2dn(keyword))\n",
        "    print(portugal3dnoticias(keyword))"
      ],
      "metadata": {
        "id": "iuNeX0gSeEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IT_keyword_list = [\"gdpr\", \"Codice in materia di protezione dei dati personali\", \"protezione dei dati\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in IT_keyword_list:\n",
        "    print(italy1repubblica(keyword))\n",
        "    #print(italy2today(keyword)) # EXCEPTION: apparently only this news site has big trouble in searching for German terms, that's why please run it afterwards with its separate keyword_list2\n",
        "    print(italy3ilpost(keyword))"
      ],
      "metadata": {
        "id": "MG7cF6vCd2gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SL_keyword_list = [\"gdpr\", \"Zàkon o ochrane osobných údajov\", \"súkromia\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in SL_keyword_list:\n",
        "    print(slovakia1topky(keyword))\n",
        "    print(slovakia2teraz(keyword))\n",
        "    print(slovakia3aktuality(keyword)) "
      ],
      "metadata": {
        "id": "4pD3Ha5MeHQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BL_keyword_list = [\"gdpr\", \"Закон за защита на личните данни\", \"поверителност\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in BL_keyword_list:\n",
        "    print(bulgaria1vesiti(keyword))\n",
        "    print(bulgaria2chasa(keyword))\n",
        "    print(bulgaria3blitz(keyword))\n",
        "    #print(bulgaria4fakti(keyword))"
      ],
      "metadata": {
        "id": "5nFf8Mz3eRhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GR_keyword_list = [\"gdpr\", \"μυστικότητα\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in GR_keyword_list:\n",
        "    print(greece1protothema(keyword))\n",
        "    print(greece2in(keyword))\n",
        "    print(greece3newsit(keyword))\n",
        "    #print(greece4dikaiologitika(keyword))"
      ],
      "metadata": {
        "id": "S47gtsKKwdtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HR_keyword_list = [\"gdpr\", \"Zakon o Provedbi Opće Uredbe o Zaštiti Podataka\", \"privatnost\", \"web tracking\", \"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in HR_keyword_list:\n",
        "    print(croatia1index(keyword))\n",
        "    print(croatia2tportal(keyword))\n",
        "    print(croatia3dnevno(keyword))"
      ],
      "metadata": {
        "id": "SNkOmY9neaIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IT_keyword_list = [\"gdpr\",\"web tracking\",\"online tracking\", \"digital analytics\", \"data protection authorities\"]\n",
        "\n",
        "for keyword in IT_keyword_list:\n",
        "      print(italy1repubblica(keyword))\n",
        "      print(italy2today(keyword)\n",
        "      print(italy3ilpost(keyword))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p897uPLURyMT",
        "outputId": "3b670bd9-92b4-4d5d-fe4d-492622174de7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PAGE NO : 1] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 2] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 3] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 4] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 5] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 6] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 7] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 8] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 9] [NO MORE ARTICLES FOUND]\n",
            "[Website: www.today.it] [Keyword: gdpr] [Time Taken: 131.59 sec]\n",
            "   year Number of Related News\n",
            "0  2017                      0\n",
            "1  2018                     26\n",
            "2  2019                     29\n",
            "3  2020                     25\n",
            "4  2021                     25\n",
            "5  2022                     15\n",
            "[PAGE NO : 1] [ARTICLES FOUND: 12]\n",
            "[PAGE NO : 2] [NO MORE ARTICLES FOUND]\n",
            "[Website: www.today.it] [Keyword: web tracking] [Time Taken: 133.95 sec]\n",
            "   year Number of Related News\n",
            "0  2017                      1\n",
            "1  2018                      1\n",
            "2  2019                      2\n",
            "3  2020                      2\n",
            "4  2021                      3\n",
            "5  2022                      3\n",
            "[PAGE NO : 1] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 2] [ARTICLES FOUND: 6]\n",
            "[PAGE NO : 3] [NO MORE ARTICLES FOUND]\n",
            "[Website: www.today.it] [Keyword: online tracking] [Time Taken: 136.42 sec]\n",
            "   year Number of Related News\n",
            "0  2017                      0\n",
            "1  2018                      4\n",
            "2  2019                      3\n",
            "3  2020                      5\n",
            "4  2021                      7\n",
            "5  2022                      2\n",
            "[PAGE NO : 1] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 2] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 3] [ARTICLES FOUND: 15]\n",
            "[PAGE NO : 4] [ARTICLES FOUND: 12]\n",
            "[PAGE NO : 5] [NO MORE ARTICLES FOUND]\n",
            "[Website: www.today.it] [Keyword: digital analytics] [Time Taken: 141.07 sec]\n",
            "   year Number of Related News\n",
            "0  2017                      0\n",
            "1  2018                      7\n",
            "2  2019                     13\n",
            "3  2020                     11\n",
            "4  2021                     22\n",
            "5  2022                      3\n",
            "[PAGE NO : 1] [ARTICLES FOUND: 1]\n",
            "[PAGE NO : 2] [NO MORE ARTICLES FOUND]\n",
            "[Website: www.today.it] [Keyword: data protection authorities] [Time Taken: 142.8 sec]\n",
            "   year Number of Related News\n",
            "0  2017                      0\n",
            "1  2018                      1\n",
            "2  2019                      0\n",
            "3  2020                      0\n",
            "4  2021                      0\n",
            "5  2022                      0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "keyword_scraper.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "05b68caa059241f19d4d9e71196d54d6acb07590019854b9cb64c25a94c7e7f6"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 ('scrapper')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}